{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimberleyEvans-Parker/ALittleMidnightMusic/blob/master/keras_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGESI8u2k3He",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "8dd7e110-950d-49c3-c8e0-4f3bb1da45f0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iCFRR3jp-lH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6ee199e-2e6a-4d21-da3e-e26ff3c51f33"
      },
      "source": [
        "cd /content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOmwOKmDpHSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "# sys.path.append('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls4wkymIueP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "21a49ed1-0009-482c-870c-5ba1aa42af4a"
      },
      "source": [
        "pip install mido"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mido\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.7MB/s \n",
            "\u001b[?25hInstalling collected packages: mido\n",
            "Successfully installed mido-1.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90wNLDymGM5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "429acc03-3bc9-4a86-c9b8-dccb2e983d2e"
      },
      "source": [
        "pip install madmom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting madmom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/a3/9f3de3e8068a3606331134d96b84c8db4f7624d6715be8ab3c1f56e6731d/madmom-0.16.1.tar.gz (20.0MB)\n",
            "\u001b[K     |████████████████████████████████| 20.0MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.4 in /usr/local/lib/python3.6/dist-packages (from madmom) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from madmom) (1.4.1)\n",
            "Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.6/dist-packages (from madmom) (0.29.21)\n",
            "Requirement already satisfied: mido>=1.2.8 in /usr/local/lib/python3.6/dist-packages (from madmom) (1.2.9)\n",
            "Building wheels for collected packages: madmom\n",
            "  Building wheel for madmom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for madmom: filename=madmom-0.16.1-cp36-cp36m-linux_x86_64.whl size=20940285 sha256=52f17e2efeeb09dfcaa95df86925c9d8c96a99362677b1892468320aa477ba96\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/0c/30/e0141aa75fb0a829ba5e1dca2be0860dc98502c1789616637d\n",
            "Successfully built madmom\n",
            "Installing collected packages: madmom\n",
            "Successfully installed madmom-0.16.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7AdSpcgGRHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "c392a690-dcb8-4d82-ebb7-763d6bce2404"
      },
      "source": [
        "pip install pretty_midi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretty_midi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/8e/63c6e39a7a64623a9cd6aec530070c70827f6f8f40deec938f323d7b1e15/pretty_midi-0.2.9.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.18.5)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-cp36-none-any.whl size=5591953 sha256=d226bcd6afc85637a58d772d4ff6ba45705f4fa02ccf0fed80bd2b41197b1591\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a1/c6/b5697841db1112c6e5866d75a6b6bf1bef73b874782556ba66\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: pretty-midi\n",
            "Successfully installed pretty-midi-0.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHfAe9eVpNaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAp9UsapOmrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "keras: CNN Transcription model\n",
        "\n",
        "'''\n",
        "#from __future__ import print_function\n",
        "import argparse\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#keras utils\n",
        "from keras.callbacks import Callback\n",
        "from keras import metrics\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Reshape, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, add\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, CSVLogger\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "#internal utils\n",
        "from preprocess import DataGen\n",
        "from config import load_config\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK1h9w1qrFI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def opt_thresholds(y_true,y_scores):\n",
        "    othresholds = np.zeros(y_scores.shape[1])\n",
        "    print(othresholds.shape)\n",
        "    for label, (label_scores, true_bin) in enumerate(zip(y_scores.T,y_true.T)):\n",
        "        #print label\n",
        "        precision, recall, thresholds = sklearn.metrics.precision_recall_curve(true_bin, label_scores)\n",
        "        max_f1 = 0\n",
        "        max_f1_threshold = .5\n",
        "        for r, p, t in zip(recall, precision, thresholds):\n",
        "            if p + r == 0: continue\n",
        "            if (2*p*r)/(p + r) > max_f1:\n",
        "                max_f1 = (2*p*r)/(p + r)\n",
        "                max_f1_threshold = t\n",
        "        #print label, \": \", max_f1_threshold, \"=>\", max_f1\n",
        "        othresholds[label] = max_f1_threshold\n",
        "        print(othresholds)\n",
        "    return othresholds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VmeHMkhrHc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class linear_decay(Callback):\n",
        "    '''\n",
        "        decay = decay value to subtract each epoch\n",
        "    '''\n",
        "    def __init__(self, initial_lr,epochs):\n",
        "        super(linear_decay, self).__init__()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.decay = initial_lr/epochs\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        new_lr = self.initial_lr - self.decay*epoch\n",
        "        print(\"ld: learning rate is now \"+str(new_lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVwLsgMErRhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class half_decay(Callback):\n",
        "    '''\n",
        "        decay = decay value to subtract each epoch\n",
        "    '''\n",
        "    def __init__(self, initial_lr,period):\n",
        "        super(half_decay, self).__init__()\n",
        "        self.init_lr = initial_lr\n",
        "        self.period = period\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        factor = epoch // self.period\n",
        "        lr  = self.init_lr / (2**factor)\n",
        "        print(\"hd: learning rate is now \"+str(lr))\n",
        "        K.set_value(self.model.optimizer.lr, lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUeuN5GrrW3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Threshold(Callback):\n",
        "    '''\n",
        "        decay = decay value to subtract each epoch\n",
        "    '''\n",
        "    def __init__(self, val_data):\n",
        "        super(Threshold, self).__init__()\n",
        "        self.val_data = val_data\n",
        "        _,y = val_data\n",
        "        self.othresholds = np.full(y.shape[1],0.5)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        #find optimal thresholds on validation data\n",
        "        x,y_true = self.val_data\n",
        "        y_scores = self.model.predict(x)\n",
        "        self.othresholds = opt_thresholds(y_true,y_scores)\n",
        "        y_pred = y_scores > self.othresholds\n",
        "        p,r,f,s = sklearn.metrics.precision_recall_fscore_support(y_true,y_pred,average='micro')\n",
        "        print(\"validation p,r,f,s:\")\n",
        "        print (p,r,f,s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GELtmCGru8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model():\n",
        "    inputs = Input(shape=input_shape)\n",
        "    reshape = Reshape(input_shape_channels)(inputs)\n",
        "\n",
        "    #normal convnet layer (have to do one initially to get 64 channels)\n",
        "    conv1 = Conv2D(50,(5,25),activation='tanh')(reshape)\n",
        "    do1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(1,3))(do1)\n",
        "\n",
        "    conv2 = Conv2D(50,(3,5),activation='tanh')(pool1)\n",
        "    do2 = Dropout(0.5)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(1,3))(do2)\n",
        "\n",
        "    flattened = Flatten()(pool2)\n",
        "    fc1 = Dense(1000, activation='sigmoid')(flattened)\n",
        "    do3 = Dropout(0.5)(fc1)\n",
        "\n",
        "    fc2 = Dense(200, activation='sigmoid')(do3)\n",
        "    do4 = Dropout(0.5)(fc2)\n",
        "    outputs = Dense(note_range, activation='sigmoid')(do4)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnpFHKMxr79U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_model(bin_multiple):\n",
        "\n",
        "    #input and reshape\n",
        "    inputs = Input(shape=input_shape)\n",
        "    reshape = Reshape(input_shape_channels)(inputs)\n",
        "\n",
        "    #normal convnet layer (have to do one initially to get 64 channels)\n",
        "    conv = Conv2D(64,(1,bin_multiple*note_range),padding=\"same\",activation='relu')(reshape)\n",
        "    pool = MaxPooling2D(pool_size=(1,2))(conv)\n",
        "\n",
        "    for i in range(int(np.log2(bin_multiple))-1):\n",
        "        print(i)\n",
        "        #residual block\n",
        "        bn = BatchNormalization()(pool)\n",
        "        re = Activation('relu')(bn)\n",
        "        freq_range = (bin_multiple/(2**(i+1)))*note_range\n",
        "        print(freq_range)\n",
        "        conv = Conv2D(64,(1,freq_range),padding=\"same\",activation='relu')(re)\n",
        "\n",
        "        #add and downsample\n",
        "        ad = add([pool,conv])\n",
        "        pool = MaxPooling2D(pool_size=(1,2))(ad)\n",
        "\n",
        "    flattened = Flatten()(pool)\n",
        "    fc = Dense(1024, activation='relu')(flattened)\n",
        "    do = Dropout(0.5)(fc)\n",
        "    fc = Dense(512, activation='relu')(do)\n",
        "    do = Dropout(0.5)(fc)\n",
        "    outputs = Dense(note_range, activation='sigmoid')(do)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzl9S9z1sIo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 7\n",
        "min_midi = 21\n",
        "max_midi = 108\n",
        "note_range = max_midi - min_midi + 1\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    print(\"training...\")\n",
        "    path = os.path.join('models',args['model_name'])\n",
        "    config = load_config(os.path.join(path,'config.json'))\n",
        "\n",
        "    global feature_bins\n",
        "    global input_shape\n",
        "    global input_shape_channels\n",
        "\n",
        "    bin_multiple = int(args['bin_multiple'])\n",
        "    print('bin multiple',str(np.log2(bin_multiple)))\n",
        "    feature_bins = note_range * bin_multiple\n",
        "    input_shape = (window_size,feature_bins)\n",
        "    input_shape_channels = (window_size,feature_bins,1)\n",
        "\n",
        "    #filenames\n",
        "    model_ckpt = os.path.join(path,'ckpt.h5')\n",
        "\n",
        "    #train params\n",
        "    batch_size = 256\n",
        "    epochs = 1000\n",
        "\n",
        "    trainGen = DataGen(os.path.join(path,'data','train'),batch_size,args)\n",
        "    valGen = DataGen(os.path.join(path,'data','val'),batch_size,args)\n",
        "    #valData = load_data(os.path.join(path,'data','val'))\n",
        "\n",
        "\n",
        "    if os.path.isfile(model_ckpt):\n",
        "        print('loading model')\n",
        "        model = load_model(model_ckpt)\n",
        "    else:\n",
        "        print('training new model from scratch')\n",
        "        if bool(args['residual']):\n",
        "            model = resnet_model(bin_multiple)\n",
        "        else:\n",
        "            model = baseline_model()\n",
        "\n",
        "    init_lr = float(args['init_lr'])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer=SGD(lr=init_lr,momentum=0.9))\n",
        "    model.summary()\n",
        "    plot_model(model, to_file=os.path.join(path,'model.png'))\n",
        "\n",
        "    checkpoint = ModelCheckpoint(model_ckpt, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "    early_stop = EarlyStopping(patience=5,monitor='val_loss', verbose=1, mode='min')\n",
        "    #tensorboard = TensorBoard(log_dir='./logs/baseline/', histogram_freq=250, batch_size=batch_size)\n",
        "    if args['lr_decay'] == 'linear':\n",
        "        decay = linear_decay(init_lr,epochs)\n",
        "    else:\n",
        "        decay = half_decay(init_lr,5)\n",
        "    csv_logger = CSVLogger(os.path.join(path,'training.log'))\n",
        "    #t = Threshold(valData)\n",
        "    callbacks = [checkpoint,early_stop,decay,csv_logger]\n",
        "\n",
        "    history = model.fit_generator(trainGen.next(),trainGen.steps(), epochs=epochs,\n",
        "              verbose=1,validation_data=valGen.next(),validation_steps=valGen.steps(),callbacks=callbacks)\n",
        "\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    '''plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig('baseline/acc.png')'''\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig('baseline/loss.png')\n",
        "\n",
        "    #test\n",
        "    testGen = DataGen(os.path.join(path,'data','test'),batch_size,args)\n",
        "\n",
        "    res = model.evaluate_generator(testGen.next(),steps=testGen.steps())\n",
        "    print(model.metrics_names)\n",
        "    print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUSNrxUnsjgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "3c71a698-3590-4508-c225-eca2a307db73"
      },
      "source": [
        "def main():\n",
        "    print(\"maining...\")\n",
        "    #train\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Preprocess MIDI/Audio file pairs into ingestible data')\n",
        "    parser.add_argument('model_name',\n",
        "                        help='Path to the model directory where data should reside')\n",
        "    # parser.add_argument('-f')\n",
        "\n",
        "    args = vars(parser.parse_args())\n",
        "    train(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maining...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] model_name\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZuYY7b6nHpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "582e7513-4a00-4caa-9e80-655f88c6007a"
      },
      "source": [
        "def opt_thresholds(y_true,y_scores):\n",
        "    othresholds = np.zeros(y_scores.shape[1])\n",
        "    print(othresholds.shape)\n",
        "    for label, (label_scores, true_bin) in enumerate(zip(y_scores.T,y_true.T)):\n",
        "        #print label\n",
        "        precision, recall, thresholds = sklearn.metrics.precision_recall_curve(true_bin, label_scores)\n",
        "        max_f1 = 0\n",
        "        max_f1_threshold = .5\n",
        "        for r, p, t in zip(recall, precision, thresholds):\n",
        "            if p + r == 0: continue\n",
        "            if (2*p*r)/(p + r) > max_f1:\n",
        "                max_f1 = (2*p*r)/(p + r)\n",
        "                max_f1_threshold = t\n",
        "        #print label, \": \", max_f1_threshold, \"=>\", max_f1\n",
        "        othresholds[label] = max_f1_threshold\n",
        "        print(othresholds)\n",
        "    return othresholds\n",
        "\n",
        "class linear_decay(Callback):\n",
        "    '''\n",
        "        decay = decay value to subtract each epoch\n",
        "    '''\n",
        "    def __init__(self, initial_lr,epochs):\n",
        "        super(linear_decay, self).__init__()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.decay = initial_lr/epochs\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        new_lr = self.initial_lr - self.decay*epoch\n",
        "        print(\"ld: learning rate is now \"+str(new_lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "class half_decay(Callback):\n",
        "    '''\n",
        "        decay = decay value to subtract each epoch\n",
        "    '''\n",
        "    def __init__(self, initial_lr,period):\n",
        "        super(half_decay, self).__init__()\n",
        "        self.init_lr = initial_lr\n",
        "        self.period = period\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        factor = epoch // self.period\n",
        "        lr  = self.init_lr / (2**factor)\n",
        "        print(\"hd: learning rate is now \"+str(lr))\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "class Threshold(Callback):\n",
        "    '''\n",
        "        decay = decay value to subtract each epoch\n",
        "    '''\n",
        "    def __init__(self, val_data):\n",
        "        super(Threshold, self).__init__()\n",
        "        self.val_data = val_data\n",
        "        _,y = val_data\n",
        "        self.othresholds = np.full(y.shape[1],0.5)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        #find optimal thresholds on validation data\n",
        "        x,y_true = self.val_data\n",
        "        y_scores = self.model.predict(x)\n",
        "        self.othresholds = opt_thresholds(y_true,y_scores)\n",
        "        y_pred = y_scores > self.othresholds\n",
        "        p,r,f,s = sklearn.metrics.precision_recall_fscore_support(y_true,y_pred,average='micro')\n",
        "        print(\"validation p,r,f,s:\")\n",
        "        print (p,r,f,s)\n",
        "\n",
        "def baseline_model():\n",
        "    inputs = Input(shape=input_shape)\n",
        "    reshape = Reshape(input_shape_channels)(inputs)\n",
        "\n",
        "    #normal convnet layer (have to do one initially to get 64 channels)\n",
        "    conv1 = Conv2D(50,(5,25),activation='tanh')(reshape)\n",
        "    do1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(1,3))(do1)\n",
        "\n",
        "    conv2 = Conv2D(50,(3,5),activation='tanh')(pool1)\n",
        "    do2 = Dropout(0.5)(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(1,3))(do2)\n",
        "\n",
        "    flattened = Flatten()(pool2)\n",
        "    fc1 = Dense(1000, activation='sigmoid')(flattened)\n",
        "    do3 = Dropout(0.5)(fc1)\n",
        "\n",
        "    fc2 = Dense(200, activation='sigmoid')(do3)\n",
        "    do4 = Dropout(0.5)(fc2)\n",
        "    outputs = Dense(note_range, activation='sigmoid')(do4)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def resnet_model(bin_multiple):\n",
        "\n",
        "    #input and reshape\n",
        "    inputs = Input(shape=input_shape)\n",
        "    reshape = Reshape(input_shape_channels)(inputs)\n",
        "\n",
        "    #normal convnet layer (have to do one initially to get 64 channels)\n",
        "    conv = Conv2D(64,(1,bin_multiple*note_range),padding=\"same\",activation='relu')(reshape)\n",
        "    pool = MaxPooling2D(pool_size=(1,2))(conv)\n",
        "\n",
        "    for i in range(int(np.log2(bin_multiple))-1):\n",
        "        print(i)\n",
        "        #residual block\n",
        "        bn = BatchNormalization()(pool)\n",
        "        re = Activation('relu')(bn)\n",
        "        freq_range = (bin_multiple/(2**(i+1)))*note_range\n",
        "        print(freq_range)\n",
        "        conv = Conv2D(64,(1,freq_range),padding=\"same\",activation='relu')(re)\n",
        "\n",
        "        #add and downsample\n",
        "        ad = add([pool,conv])\n",
        "        pool = MaxPooling2D(pool_size=(1,2))(ad)\n",
        "\n",
        "    flattened = Flatten()(pool)\n",
        "    fc = Dense(1024, activation='relu')(flattened)\n",
        "    do = Dropout(0.5)(fc)\n",
        "    fc = Dense(512, activation='relu')(do)\n",
        "    do = Dropout(0.5)(fc)\n",
        "    outputs = Dense(note_range, activation='sigmoid')(do)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "window_size = 7\n",
        "min_midi = 21\n",
        "max_midi = 108\n",
        "note_range = max_midi - min_midi + 1\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    print(\"training...\")\n",
        "    path = os.path.join('models',args['model_name'])\n",
        "    config = load_config(os.path.join(path,'model.json'))\n",
        "\n",
        "    global feature_bins\n",
        "    global input_shape\n",
        "    global input_shape_channels\n",
        "\n",
        "    bin_multiple = int(args['bin_multiple'])\n",
        "    print('bin multiple',str(np.log2(bin_multiple)))\n",
        "    feature_bins = note_range * bin_multiple\n",
        "    input_shape = (window_size,feature_bins)\n",
        "    input_shape_channels = (window_size,feature_bins,1)\n",
        "\n",
        "    #filenames\n",
        "    model_ckpt = os.path.join(path,'ckpt.h5')\n",
        "\n",
        "    #train params\n",
        "    batch_size = 256\n",
        "    epochs = 1000\n",
        "\n",
        "    trainGen = DataGen(os.path.join(path,'data','train'),batch_size,args)\n",
        "    valGen = DataGen(os.path.join(path,'data','val'),batch_size,args)\n",
        "    #valData = load_data(os.path.join(path,'data','val'))\n",
        "\n",
        "\n",
        "    if os.path.isfile(model_ckpt):\n",
        "        print('loading model')\n",
        "        model = load_model(model_ckpt)\n",
        "    else:\n",
        "        print('training new model from scratch')\n",
        "        if bool(args['residual']):\n",
        "            model = resnet_model(bin_multiple)\n",
        "        else:\n",
        "            model = baseline_model()\n",
        "\n",
        "    init_lr = float(args['init_lr'])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer=SGD(lr=init_lr,momentum=0.9))\n",
        "    model.summary()\n",
        "    plot_model(model, to_file=os.path.join(path,'model.png'))\n",
        "\n",
        "    checkpoint = ModelCheckpoint(model_ckpt, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "    early_stop = EarlyStopping(patience=5,monitor='val_loss', verbose=1, mode='min')\n",
        "    #tensorboard = TensorBoard(log_dir='./logs/baseline/', histogram_freq=250, batch_size=batch_size)\n",
        "    if args['lr_decay'] == 'linear':\n",
        "        decay = linear_decay(init_lr,epochs)\n",
        "    else:\n",
        "        decay = half_decay(init_lr,5)\n",
        "    csv_logger = CSVLogger(os.path.join(path,'training.log'))\n",
        "    #t = Threshold(valData)\n",
        "    callbacks = [checkpoint,early_stop,decay,csv_logger]\n",
        "\n",
        "    history = model.fit_generator(trainGen.next(),trainGen.steps(), epochs=epochs,\n",
        "              verbose=1,validation_data=valGen.next(),validation_steps=valGen.steps(),callbacks=callbacks)\n",
        "\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    '''plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig('baseline/acc.png')'''\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig('baseline/loss.png')\n",
        "\n",
        "    #test\n",
        "    testGen = DataGen(os.path.join(path,'data','test'),batch_size,args)\n",
        "\n",
        "    res = model.evaluate_generator(testGen.next(),steps=testGen.steps())\n",
        "    print(model.metrics_names)\n",
        "    print(res)\n",
        "def main():\n",
        "    print(\"maining...\")\n",
        "    #train\n",
        "    # parser = argparse.ArgumentParser(\n",
        "    #     description='Preprocess MIDI/Audio file pairs into ingestible data')\n",
        "    # parser.add_argument('model_name',\n",
        "    #                     help='Path to the model directory where data should reside')\n",
        "    # parser.add_argument('-f')\n",
        "\n",
        "    # args = vars(parser.parse_args())\n",
        "    # train(args)\n",
        "    train({\"model_name\": \"new\", \"bin_multiple\": 4, \"init_ltr\": 0.1, \"lr_decay\": \"half\", \"residual\": True, \"spec_type\": \"cqt\"})\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maining...\n",
            "training...\n",
            "bin multiple 2.0\n",
            "initializing gen for models/new/data/train\n",
            "Bounce\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4de5b9a44776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-4de5b9a44776>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;31m# args = vars(parser.parse_args())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# train(args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"model_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"new\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bin_multiple\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"init_ltr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr_decay\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"half\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"residual\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spec_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cqt\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4de5b9a44776>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mtrainGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mvalGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m#valData = load_data(os.path.join(path,'data','val'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid-master/preprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dirpath, batch_size, args, num_files)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmmdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmmdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspe\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m#print cnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid-master/preprocess.py\u001b[0m in \u001b[0;36mreadmm\u001b[0;34m(d, args)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mopath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'output.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mmmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 321 into shape (7,352)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9sZcLjaGbkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "d5396e37-bc70-4eb5-ed77-9b34072ad2cc"
      },
      "source": [
        "%tb\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4de5b9a44776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-4de5b9a44776>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;31m# args = vars(parser.parse_args())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# train(args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"model_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"new\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bin_multiple\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"init_ltr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr_decay\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"half\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"residual\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spec_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cqt\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4de5b9a44776>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mtrainGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mvalGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m#valData = load_data(os.path.join(path,'data','val'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid-master/preprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dirpath, batch_size, args, num_files)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting with '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmdirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_file_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_file_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mmmdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmdirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_file_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4v_0jOP29p0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ac376217-9d4e-4dfa-d7ab-ee842b7d592c"
      },
      "source": [
        "!python keras_train.py new\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2020-07-28 22:48:37.459279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"keras_train.py\", line 267, in <module>\n",
            "    main()\n",
            "  File \"keras_train.py\", line 263, in main\n",
            "    train(args)\n",
            "  File \"keras_train.py\", line 173, in train\n",
            "    config = load_config(os.path.join(path,'config.py'))\n",
            "  File \"/content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid-master/config.py\", line 12, in load_config\n",
            "    with open(json_fn, 'r') as infile:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'models/new/config.py'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}